# 网络爬虫

在学习网络爬虫之前需要具备一定的网络知识基础，理解HTTP/IP协议；网络爬虫需要完成数据请求、数据提取以及数据存储三个输要任务，其技术重点在于数据请求与数据提取。

## 数据请求

在Python中，我们可以通过标准库urllib请求数据，也可以使用第三方库Request请求数据。

### Urllib

urllib是Python内置的网络请求库，其中包含了parse、request、response、error、robotparser五个模块分别用于url解析、url请求、
url响应对象、url异常处理、解析robot.txt文件。

* 直接使用默认get方式请求url数据:

  ```python
  from urllib import request
  url = "http://www.baidu.com"
  req = request.urlopen(url)            # 使用urlopen()请求数据,返回req为服务器的响应。
  print(req.status)                     # 打印响应状态码。如果200为成功。
  html = req.read().decode()            # 使用read()读取响应内容，内容为字节流需要解码成字符串。
  ```

* 在使用decode解码相应内容时，如果失败，并字节流是以`/x1f/x8b/x08`开头的，说明经过gzip压缩，需要解压，这时可能还会遇到uft-8解码失败，需要跟换为gbk解码：

  ```python
  import gzip
  from io import BytesIO
  from urllib import request
  url = "http://www.jjwxc.net/"
  req = request.urlopen(url)
  con = BytesIO(req.read())
  req = gzip.GzipFile(fileobj=con)    # 进行gzip解压。
  html = req.read().decode("gbk")     # 进行gbk解码。
  print(html)
  ```

* 有些网站禁止爬虫访问，需要请求头来伪装正常用户，一般添加user-agent即可：

  ```python
  from urllib import request
  headers = {
      "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36"
  }
  url = "https://www.cnblogs.com/"
  urler = request.Request(url, headers=headers)
  req = request.urlopen(url)
  print(req.status)
  html = req.read().decode()
  print(html)
  ```

* 有些网站在SSL证书验证不通过时或者操作不信任的网站时需要忽略SSL证书错误。

  ```python
  
  ```

### Request

### selenium

selenium是用于自动化测试的浏览器驱动模块，用于驱动浏览器进行相关操作。

## 数据提取

即从请求响应的数据中获取我们所需的数据。

### BeautifulSoup

### re

re是一个提供正则表达式的Python库。

* 【function】生成一个正则表达式对象：

  ```python
  patterner = re.compile(pattern,flags=0)
  ```

* 【function】扫描字符串，查找第一个字符串，并返回匹配对象：

  ```python
  re.search(pattern,string,flags=0)
  ```

* 【function】如果字符串开头的零个或多个字符与正则表达式模式匹配，则返回一个对应的match对象：

  ```python
  re.match(pattern, string, flags=0)
  ```

### JSON

### XPath

### PyQuery

### 直接获取

## 数据存储

使用cvs、xml、json、MySQL、mongoDB、Redis来存储数据。

## 爬虫框架

### pyspider

### scripy
